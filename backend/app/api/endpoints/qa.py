from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
from ...core.rte import get_completion_from_messages, rte_from_text
import json
import asyncio
import re

router = APIRouter()

class Message(BaseModel):
    role: str
    content: str
    timestamp: Optional[str] = None

class QARequest(BaseModel):
    message: str
    system_prompt: str = ""
    conversation_history: Optional[List[Message]] = []

@router.post("")
async def qa(request: QARequest):
    """
    QA API endpoint - æ”¯æŒå…³ç³»æŠ½å–
    """
    try:
        messages = []
        if request.system_prompt:
            messages.append({'role': 'system', 'content': request.system_prompt})
        
        # æ·»åŠ å¯¹è¯å†å²
        if request.conversation_history:
            for msg in request.conversation_history:
                messages.append({'role': msg.role, 'content': msg.content})
            print(f"ğŸ“š æ·»åŠ å†å²å¯¹è¯: {len(request.conversation_history)} æ¡æ¶ˆæ¯")
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({'role': 'user', 'content': request.message})
        
        print(f"ğŸš€ æ”¶åˆ°QAè¯·æ±‚ï¼Œæ€»æ¶ˆæ¯æ•°: {len(messages)}")
        message_overview = []
        for m in messages:
            message_overview.append(f"{m['role']}: {m['content'][:30]}...")
        print(f"ğŸ“œ æ¶ˆæ¯æ¦‚è§ˆ: {message_overview}")
        
        # è·å–AIå›ç­”
        answer = get_completion_from_messages(messages, model="deepseek-chat", temperature=0.7)
        
        # è¿›è¡Œå…³ç³»æŠ½å–
        triplets = rte_from_text(answer)
        
        # è¿”å›ç»“æœ
        return {
            "answer": answer, 
            "triplets": triplets,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ QA APIé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/stream")
async def qa_stream(request: QARequest):
    """
    æµå¼QA API endpoint - æ”¯æŒå¯¹è¯å†å²ä¸Šä¸‹æ–‡
    """
    async def generate():
        try:
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if request.system_prompt:
                messages.append({'role': 'system', 'content': request.system_prompt})
                print(f"ğŸ”§ æ·»åŠ ç³»ç»Ÿæç¤ºè¯: {request.system_prompt[:50]}...")
            else:
                # é»˜è®¤ç³»ç»Ÿæç¤ºè¯ï¼Œå¼ºè°ƒä¸“æ³¨å½“å‰é—®é¢˜
                default_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚è¯·ä¸“æ³¨å›ç­”ç”¨æˆ·å½“å‰æå‡ºçš„é—®é¢˜ï¼Œæä¾›å‡†ç¡®ã€ç®€æ´ã€æœ‰ç”¨çš„å›ç­”ã€‚ä¸è¦é‡å¤ä¹‹å‰å·²ç»å›ç­”è¿‡çš„å†…å®¹ï¼Œä¸“æ³¨äºå½“å‰çš„é—®é¢˜ã€‚"
                messages.append({'role': 'system', 'content': default_prompt})
                print("ğŸ”§ ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤ºè¯")
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆé‡è¦ï¼šä¿æŒä¸Šä¸‹æ–‡ï¼‰
            if request.conversation_history:
                for msg in request.conversation_history:
                    messages.append({'role': msg.role, 'content': msg.content})
                print("ğŸ“š æ·»åŠ å†å²å¯¹è¯")
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆé‡è¦ï¼šç¡®ä¿å½“å‰æ¶ˆæ¯è¢«åŒ…å«ï¼‰
            messages.append({'role': 'user', 'content': request.message})
            
            print(f"ğŸš€ æ”¶åˆ°æµå¼è¯·æ±‚ï¼Œæ€»æ¶ˆæ¯æ•°: {len(messages)}")
            print(f"ğŸ“ å½“å‰ç”¨æˆ·æ¶ˆæ¯: {request.message[:100]}...")
            
            # æ‰“å°å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print("ğŸ“‹ å®Œæ•´å¯¹è¯ä¸Šä¸‹æ–‡:")
            for i, msg in enumerate(messages):
                role_emoji = "ğŸ¤–" if msg['role'] == 'assistant' else "ğŸ‘¤" if msg['role'] == 'user' else "âš™ï¸"
                content_preview = msg['content'][:80] + "..." if len(msg['content']) > 80 else msg['content']
                print(f"  {i+1}. {role_emoji} {msg['role']}: {content_preview}")
            
            # è·å–AIå›ç­”
            answer = get_completion_from_messages(messages, model="deepseek-chat", temperature=0.7)
            
            print(f"âœ… è·å¾—AIå›ç­”: {len(answer)} å­—ç¬¦")
            print(f"ğŸ“„ å›ç­”é¢„è§ˆ: {answer[:100]}...")
            
            # æ”¹è¿›çš„æµå¼è¾“å‡º - æŒ‰åˆç†çš„å—è¾“å‡º
            chunks = split_text_for_streaming(answer)
            
            for i, chunk_text in enumerate(chunks):
                chunk = {
                    "content": chunk_text,
                    "type": "text"
                }
                yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                await asyncio.sleep(0.05)  # é€‚ä¸­çš„å»¶è¿Ÿ
            
            # åœ¨æµå¼è¾“å‡ºå®Œæˆåè¿›è¡Œå…³ç³»æŠ½å–ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰
            try:
                print("ğŸ”„ å¼€å§‹å¯¹AIå›å¤è¿›è¡Œå…³ç³»æŠ½å–...")
                triplets = rte_from_text(answer)
                
                # å‘é€ä¸‰å…ƒç»„æ•°æ®
                if triplets:
                    triplets_chunk = {
                        "content": "",
                        "type": "triplets",
                        "triplets": triplets
                    }
                    yield f"data: {json.dumps(triplets_chunk, ensure_ascii=False)}\n\n"
                    print(f"ğŸ“Š å‘é€äº† {len(triplets)} ä¸ªä¸‰å…ƒç»„ç”¨äºåŠ¨æ€å›¾ç”Ÿæˆ")
                else:
                    print("âš ï¸ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ä¸‰å…ƒç»„")
            except Exception as rte_error:
                print(f"âŒ å…³ç³»æŠ½å–å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹: {rte_error}")
                # RTEå¤±è´¥ä¸åº”è¯¥ä¸­æ–­æ•´ä¸ªå“åº”æµ
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: [DONE]\n\n"
            print(f"ğŸ‰ æµå¼è¾“å‡ºå®Œæˆï¼Œå…±å‘é€ {len(chunks)} ä¸ªæ•°æ®å—")
            
        except Exception as e:
            print(f"âŒ æµå¼APIé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            error_chunk = {
                "type": "error",
                "error": str(e),
                "content": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            }
            yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate(), media_type="text/plain")

def split_text_for_streaming(text):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆåˆé€‚çš„æµå¼è¾“å‡ºå—
    """
    if not text:
        return []
    
    chunks = []
    
    # æŒ‰è¡Œåˆ†å‰²
    lines = text.split('\n')
    
    for line in lines:
        if not line.strip():
            chunks.append('\n')
            continue
            
        # å¯¹äºè¾ƒé•¿çš„è¡Œï¼ŒæŒ‰å¥å­æˆ–çŸ­è¯­åˆ†å‰²
        if len(line) > 50:
            # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', line)
            current_chunk = ""
            
            for i, part in enumerate(sentences):
                current_chunk += part
                
                # å¦‚æœæ˜¯æ ‡ç‚¹ç¬¦å·æˆ–ç§¯ç´¯äº†è¶³å¤Ÿçš„å­—ç¬¦ï¼Œè¾“å‡ºä¸€ä¸ªå—
                if part in 'ã€‚ï¼ï¼Ÿ.!?' or len(current_chunk) >= 20:
                    if current_chunk.strip():
                        chunks.append(current_chunk)
                        current_chunk = ""
            
            # å¤„ç†å‰©ä½™çš„å†…å®¹
            if current_chunk.strip():
                chunks.append(current_chunk)
                
            chunks.append('\n')
        else:
            # çŸ­è¡Œç›´æ¥è¾“å‡º
            chunks.append(line + '\n')
    
    # è¿‡æ»¤ç©ºå—å¹¶åˆå¹¶è¿‡çŸ­çš„å—
    filtered_chunks = []
    for chunk in chunks:
        if chunk.strip() or chunk == '\n':
            filtered_chunks.append(chunk)
    
    return filtered_chunks 